{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MandbeZ/TFM_sequia/blob/main/notebooks/4.0.%20Modelo_Univariante_ARIMA_RF__SPI_SPEI(Por_cluster).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i47hVf8HoVhg"
      },
      "source": [
        "Instalación de SKtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vkO36W0ioP_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e4e9aa3-4365-4fc5-aa18-df91c7dcba2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sktime\n",
            "  Downloading sktime-0.10.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.22,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.21.5)\n",
            "Requirement already satisfied: scipy<1.8.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.4.1)\n",
            "Collecting statsmodels>=0.12.1\n",
            "  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.0.2)\n",
            "Collecting numba>=0.53\n",
            "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 66.7 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.13\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: pandas<1.5.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.3.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.13->sktime) (1.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (57.4.0)\n",
            "Collecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 15 kB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<1.5.0,>=1.1.0->sktime) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels>=0.12.1->sktime) (3.0.7)\n",
            "Installing collected packages: llvmlite, statsmodels, numba, deprecated, sktime\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed deprecated-1.2.13 llvmlite-0.38.0 numba-0.55.1 sktime-0.10.1 statsmodels-0.13.2\n",
            "Requirement already satisfied: sktime[all_extras] in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pandas<1.5.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (1.3.5)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (1.2.13)\n",
            "Requirement already satisfied: numpy<1.22,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (1.21.5)\n",
            "Requirement already satisfied: scipy<1.8.0 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (1.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (0.13.2)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (0.55.1)\n",
            "Collecting pyod>=0.8.0\n",
            "  Downloading pyod-0.9.7.tar.gz (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting prophet>=1.0\n",
            "  Downloading prophet-1.0.1.tar.gz (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting tsfresh>=0.17.0\n",
            "  Downloading tsfresh-0.19.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (0.11.2)\n",
            "Collecting esig==0.9.7\n",
            "  Downloading esig-0.9.7-cp37-cp37m-manylinux2010_x86_64.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 67.4 MB/s \n",
            "\u001b[?25hCollecting tbats>=1.1.0\n",
            "  Downloading tbats-1.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting matplotlib>=3.3.2\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 49.8 MB/s \n",
            "\u001b[?25hCollecting dtw-python\n",
            "  Downloading dtw_python-1.1.12-cp37-cp37m-manylinux2010_x86_64.whl (580 kB)\n",
            "\u001b[K     |████████████████████████████████| 580 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting pmdarima!=1.8.1,>=1.8.0\n",
            "  Downloading pmdarima-1.8.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 52.5 MB/s \n",
            "\u001b[?25hCollecting scikit-posthocs>=0.6.5\n",
            "  Downloading scikit-posthocs-0.6.7.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hcrystalball>=0.1.9\n",
            "  Downloading hcrystalball-0.1.10-py2.py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting stumpy>=1.5.1\n",
            "  Downloading stumpy-1.10.2-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pystan==2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from sktime[all_extras]) (2.19.1.1)\n",
            "Requirement already satisfied: Cython!=0.25.1,>=0.22 in /usr/local/lib/python3.7/dist-packages (from pystan==2.19.1.1->sktime[all_extras]) (0.29.28)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.13->sktime[all_extras]) (1.13.3)\n",
            "Collecting workalendar>=10.1\n",
            "  Downloading workalendar-16.3.0-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 72.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->sktime[all_extras]) (1.3.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.29.1-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->sktime[all_extras]) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->sktime[all_extras]) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->sktime[all_extras]) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->sktime[all_extras]) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->sktime[all_extras]) (3.0.7)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime[all_extras]) (0.38.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime[all_extras]) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=1.1.0->sktime[all_extras]) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima!=1.8.1,>=1.8.0->sktime[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from pmdarima!=1.8.1,>=1.8.0->sktime[all_extras]) (1.24.3)\n",
            "Collecting cmdstanpy==0.9.68\n",
            "  Downloading cmdstanpy-0.9.68-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.7/dist-packages (from prophet>=1.0->sktime[all_extras]) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from prophet>=1.0->sktime[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from prophet>=1.0->sktime[all_extras]) (0.10.5.2)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.7/dist-packages (from prophet>=1.0->sktime[all_extras]) (1.2)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from prophet>=1.0->sktime[all_extras]) (4.63.0)\n",
            "Collecting ujson\n",
            "  Downloading ujson-5.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->prophet>=1.0->sktime[all_extras]) (0.5.11)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->prophet>=1.0->sktime[all_extras]) (2.2.3)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->prophet>=1.0->sktime[all_extras]) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->prophet>=1.0->sktime[all_extras]) (1.15.0)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->prophet>=1.0->sktime[all_extras]) (4.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime[all_extras]) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime[all_extras]) (0.5.2)\n",
            "Collecting scipy<1.8.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh>=0.17.0->sktime[all_extras]) (2.23.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from tsfresh>=0.17.0->sktime[all_extras]) (1.3.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting matrixprofile<2.0.0,>=1.1.10\n",
            "  Downloading matrixprofile-1.1.10-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh>=0.17.0->sktime[all_extras]) (2.12.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh>=0.17.0->sktime[all_extras]) (0.11.2)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 80.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (2.1.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2022.1.1-py3-none-any.whl (830 kB)\n",
            "\u001b[K     |████████████████████████████████| 830 kB 71.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (5.4.8)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (7.1.2)\n",
            "  Downloading distributed-2022.1.0-py3-none-any.whl (822 kB)\n",
            "\u001b[K     |████████████████████████████████| 822 kB 37.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.12.0-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (1.7.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (5.1.1)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (1.0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (3.13)\n",
            "Collecting cloudpickle\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 65.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.1-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 72.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 73.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.10.0-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 67.7 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 68.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n",
            "\u001b[K     |████████████████████████████████| 779 kB 74.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n",
            "\u001b[K     |████████████████████████████████| 778 kB 75.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n",
            "\u001b[K     |████████████████████████████████| 776 kB 55.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 60.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 69.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 62.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 76.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 74.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n",
            "\u001b[K     |████████████████████████████████| 715 kB 72.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n",
            "\u001b[K     |████████████████████████████████| 705 kB 78.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n",
            "\u001b[K     |████████████████████████████████| 699 kB 77.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 75.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 71.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 72.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 71.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 75.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 72.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 74.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 56.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n",
            "\u001b[K     |████████████████████████████████| 656 kB 77.7 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.2\n",
            "  Downloading protobuf-3.11.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 63.1 MB/s \n",
            "\u001b[?25hCollecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh>=0.17.0->sktime[all_extras]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh>=0.17.0->sktime[all_extras]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh>=0.17.0->sktime[all_extras]) (3.0.4)\n",
            "Collecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting lunardate\n",
            "  Downloading lunardate-0.2.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from workalendar>=10.1->hcrystalball>=0.1.9->sktime[all_extras]) (4.11.2)\n",
            "Collecting pyluach\n",
            "  Downloading pyluach-1.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh>=0.17.0->sktime[all_extras]) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->workalendar>=10.1->hcrystalball>=0.1.9->sktime[all_extras]) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->workalendar>=10.1->hcrystalball>=0.1.9->sktime[all_extras]) (3.7.0)\n",
            "Building wheels for collected packages: prophet, pyod, scikit-posthocs\n",
            "  Building wheel for prophet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prophet: filename=prophet-1.0.1-py3-none-any.whl size=6640476 sha256=e14177969bc05ddc98dffcd7d12d675be67b56aea7a01abf42be2d5a271bbcde\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/a0/1a/02c9ec9e3e9de6bdbb3d769d11992a6926889d71567d6b9b67\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-0.9.7-py3-none-any.whl size=136279 sha256=c4df25a53b615da11847791b65c61300b3b06b66a3dedf20e0eb3eb60f71d5c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/14/ae/60cbb36511e59bc12f8f0883805f586db3b315972b54865d33\n",
            "  Building wheel for scikit-posthocs (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-posthocs: filename=scikit_posthocs-0.6.7-py3-none-any.whl size=37903 sha256=ca39adb5f050d66c6a0590afe597c0c33264e2f27a4aa01ba56318505f392316\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/21/e6/f39794d4a6ee3d3cc5146ca80b5cd949452ad4a8fde9f6b9fc\n",
            "Successfully built prophet pyod scikit-posthocs\n",
            "Installing collected packages: scipy, locket, fonttools, ujson, pyluach, protobuf, partd, matplotlib, lunardate, fsspec, cloudpickle, backports.zoneinfo, workalendar, stumpy, pmdarima, matrixprofile, distributed, cmdstanpy, tsfresh, tbats, scikit-posthocs, pyod, prophet, hcrystalball, esig, dtw-python\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: cmdstanpy\n",
            "    Found existing installation: cmdstanpy 0.9.5\n",
            "    Uninstalling cmdstanpy-0.9.5:\n",
            "      Successfully uninstalled cmdstanpy-0.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow-metadata 1.7.0 requires protobuf<4,>=3.13, but you have protobuf 3.11.2 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "googleapis-common-protos 1.55.0 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "fbprophet 0.7.1 requires cmdstanpy==0.9.5, but you have cmdstanpy 0.9.68 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed backports.zoneinfo-0.2.1 cloudpickle-2.0.0 cmdstanpy-0.9.68 distributed-2.30.1 dtw-python-1.1.12 esig-0.9.7 fonttools-4.29.1 fsspec-2022.2.0 hcrystalball-0.1.10 locket-0.2.1 lunardate-0.2.0 matplotlib-3.5.1 matrixprofile-1.1.10 partd-1.2.0 pmdarima-1.8.5 prophet-1.0.1 protobuf-3.11.2 pyluach-1.4.0 pyod-0.9.7 scikit-posthocs-0.6.7 scipy-1.7.3 stumpy-1.10.2 tbats-1.1.0 tsfresh-0.19.0 ujson-5.1.0 workalendar-16.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: esig in /usr/local/lib/python3.7/dist-packages (0.9.7)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from esig) (1.21.5)\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n",
            "Collecting matplotlib==3.1.1\n",
            "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.1) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.1\n",
            "    Uninstalling matplotlib-3.5.1:\n",
            "      Successfully uninstalled matplotlib-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fbprophet 0.7.1 requires cmdstanpy==0.9.5, but you have cmdstanpy 0.9.68 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install sktime\n",
        "%pip install sktime[all_extras]\n",
        "%pip install esig\n",
        "%pip install utils\n",
        "%pip install matplotlib==3.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uykek5o7IH_B"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Fnl9K5jkW07"
      },
      "outputs": [],
      "source": [
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
        "simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "# simplefilter(action=\"ignore\", category=ModelFitWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3oyjVVo4cm"
      },
      "source": [
        "Importar librerías básicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1CjfvPbmpAgU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sktime.forecasting.model_selection import temporal_train_test_split\n",
        "from sktime.forecasting.compose import make_reduction\n",
        "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error,\\\n",
        "                                                   mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Naive Forecast / Pronóstico Ingenuo\n",
        "from sktime.forecasting.naive import NaiveForecaster\n",
        "\n",
        "# AutoARIMA\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "\n",
        "# AutoETS\n",
        "from sktime.forecasting.ets import AutoETS\n",
        "\n",
        "# Algoritmos de Regresión de sklearn\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0-XkFQ5WCbJW"
      },
      "outputs": [],
      "source": [
        "t0 = pd.Timestamp.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jxdXKhDeZH07"
      },
      "outputs": [],
      "source": [
        "#Definición de funciones\n",
        "\n",
        "def evaluar_modelo(test,pred):\n",
        "  print('MAPE:',mean_absolute_percentage_error(test, pred, symmetric=False)) \n",
        "  print('SMAPE:',mean_absolute_percentage_error(test, pred)) #symmetric default=True\n",
        "  print('MAE:',mean_absolute_error(test, pred) )\n",
        "  print('RMSE:',mean_squared_error(test, pred, square_root=True) ) # square_root=True RMSE , false MSE)\n",
        "  print('MSE:',mean_squared_error(test, pred, square_root=False) ) # square_root=True RMSE , false MSE)\n",
        "\n",
        "def graficar_modelo(train,test,pred,titulo='Modelo',inicio_serie=200,etiqy='Valor'):\n",
        "  plot_series(train[inicio_serie:], test, pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
        "  plt.axhline(y=0.5, color='r', linestyle='dashed')\n",
        "  plt.title(titulo, c='darkblue', fontweight='bold', fontsize=15)\n",
        "  plt.ylabel(etiqy.upper(), fontweight='bold', fontsize=12)\n",
        "  plt.show()\n",
        "  #plt.savefig('../datos/4.1.Resultados_Graficas/'+titulo.replace(' ','')+'_'+etiqy.split(' ')[0]+'.png', dpi=300)\n",
        "\n",
        "\n",
        "def graficar_pred_est(data, modelo, clust=[0,1,2,3,4], escala=[1,3,6,12]):\n",
        "  predic = data\n",
        "\n",
        "  if 'spi' in predic.columns[0] : s = 'spi'\n",
        "  else : s = 'spei'\n",
        "\n",
        "  for e in escala:\n",
        "    indice = [ind for ind in predic.columns if s+str(e) in ind]\n",
        "    print('Escala: '+str(e))\n",
        "    if not indice: \n",
        "      print(indice)\n",
        "      continue\n",
        "    else:\n",
        "      datos = cargar_datos('indices_'+s+str(e)+'.csv')\n",
        "      p_datos = procesa_datos(datos)\n",
        "      datos_normalizados=normalizar_datos(p_datos)\n",
        "\n",
        "      for c in clust:\n",
        "        \n",
        "        cad = s+str(e)+'_c'+str(c)\n",
        "        \n",
        "        if (cad not in predic.columns):\n",
        "          continue\n",
        "        else:\n",
        "          estaciones = cluster[cluster['cluster'] == c].reset_index(drop=True)\n",
        "          \n",
        "\n",
        "          for est in range(estaciones.shape[0]):\n",
        "            estacion = [col for col in datos_normalizados.columns if str(estaciones['id'][est]) in col]\n",
        "            serie = datos_normalizados.loc[:,estacion]\n",
        "            train, test = dividir_datos(serie, tamanio = tam_ypred)\n",
        "            pred = data.iloc[:,0]\n",
        "            pred.index = test.index\n",
        "            nom_est = 'Estacion '+str(estaciones.iloc[est,0])+' - '+str(estaciones.iloc[est,1]).capitalize() \n",
        "            nom_clu = 'Cluster ' + str(c) + ' - '\n",
        "\n",
        "            graficar_modelo(train, test, pred,titulo=modelo+' - '+nom_clu+nom_est, inicio_serie=0, etiqy = s+str(e)+' - Normalizado')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyHUg5UookM1"
      },
      "source": [
        "# 1. Cargar los datos de SPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c_e9pqMmobcq"
      },
      "outputs": [],
      "source": [
        "def cargar_datos(archivo):\n",
        "    data = pd.read_csv('https://raw.githubusercontent.com/MandbeZ/TFM_sequia/main/datos/spi_spei/' + archivo,  sep = ',', parse_dates=True)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAHMLwCUuJOj"
      },
      "source": [
        "# 2. Preprocesamiento de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iPVWivmZo4FD"
      },
      "outputs": [],
      "source": [
        "'''Llevar la  fecha a índice, configurar como periodo'''\n",
        "def procesa_datos(data):\n",
        "    data['fecha'] = pd.to_datetime(data['fecha'])\n",
        "    data = data.dropna()\n",
        "    data = data.set_index('fecha')\n",
        "    data.index = data.index.to_period('M')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k2F2lE-p3Y6"
      },
      "source": [
        "## Normalizar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XfzLa7RBp1Yq"
      },
      "outputs": [],
      "source": [
        "'''para mejores resultados se pueden normalizar los datos: [0,1]'''\n",
        "def normalizar_datos(data):\n",
        "    return (data - data.min()) / ( data.max() - data.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alai9CZ-CbJb"
      },
      "source": [
        "## Generar cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q7S_7aqmCbJc"
      },
      "outputs": [],
      "source": [
        "'''Devuelve un Dataframe con todas las estaciones de un determinado cluster'''\n",
        "def gen_cluster(lista_clust, clust, normalizados):\n",
        "    estaciones = lista_clust[lista_clust['cluster'] == clust]\n",
        "    nom_cols = [col for est in estaciones['id'] for col in normalizados.columns if str(est) in col]\n",
        "    datos = normalizados[nom_cols]\n",
        "    datos = datos.melt(value_name='valor').reset_index(drop=True)\n",
        "    return datos.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF6aJJEJsoBL"
      },
      "source": [
        "# 3. Dividir Datos en Entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S3Di0reDsFI0"
      },
      "outputs": [],
      "source": [
        "'''Dividir el Dataset en Entrenamiento y prueba'''\n",
        "def dividir_datos(serie, tamanio):\n",
        "    y_train, y_test = temporal_train_test_split(serie, test_size=tamanio)\n",
        "    return y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b9zJ0O_wpIT"
      },
      "source": [
        "## Definir  el horizonte de predicción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wEC4ZhXAwxVL"
      },
      "outputs": [],
      "source": [
        "'''Definir horizonte de predicción'''\n",
        "def horizon_prediccion(y_test):\n",
        "    return ForecastingHorizon(y_test.index, is_relative=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfI_u2zSvq5X"
      },
      "source": [
        "# 4. Configuración del algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CS7d72xtsnrO"
      },
      "outputs": [],
      "source": [
        "def config_algoritmo(algoritmo, estacionalidad = 12, ventana = 12, estrategia = 'mean'):\n",
        "    if algoritmo=='naive': return NaiveForecaster(strategy=estrategia, sp=estacionalidad, window_length=ventana) #strategy=\"drift\" o \"last\"  sp estacionalidad\n",
        "    elif algoritmo=='arima': return AutoARIMA(sp=estacionalidad, suppress_warnings=True) #Cambiar sp para reducir el error\n",
        "    elif algoritmo=='ets': return AutoETS(auto=True,sp=estacionalidad, n_jobs=-1) #sp=12\n",
        "\n",
        "def config_algoritmoReg(algoritmo,ventana = 12, estrategia = 'recursive',estimadores=30, vecinos=11):\n",
        "    if algoritmo=='LinearRegression':\n",
        "        reg=LinearRegression()    \n",
        "    elif algoritmo=='DecisionTreeRegressor': \n",
        "        reg=DecisionTreeRegressor()\n",
        "    elif algoritmo=='KNeighborsRegressor':\n",
        "        reg=KNeighborsRegressor(n_neighbors=vecinos)\n",
        "    elif algoritmo=='RandomForestRegressor':\n",
        "        reg=RandomForestRegressor(n_estimators=estimadores)\n",
        "    return make_reduction(reg,strategy=estrategia, window_length=ventana)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7TSsRnwyH30"
      },
      "source": [
        "# 5. Ajuste del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8ZBPx5EOygx4"
      },
      "outputs": [],
      "source": [
        "def ajuste(predictor, y_train):\n",
        "    return predictor.fit(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH9PfvQYyMFK"
      },
      "source": [
        "# 6. Predicción del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z5DLr3Aqxe2U"
      },
      "outputs": [],
      "source": [
        "def prediccion(predictor, horiz_prediccion):\n",
        "    return predictor.predict(horiz_prediccion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txXDzxqeyUiW"
      },
      "source": [
        "# 7. Evaluación del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZL20hhlUxiFO"
      },
      "outputs": [],
      "source": [
        "def evaluacion_modelo(test,pred):\n",
        "    mape = mean_absolute_percentage_error(test, pred, symmetric=False)\n",
        "    # smape = mean_absolute_percentage_error(test, pred) #symmetric default=True\n",
        "    mae = mean_absolute_error(test, pred) \n",
        "    rmse = mean_squared_error(test, pred, square_root=True) # square_root=True RMSE , false MSE)\n",
        "    mse = mean_squared_error(test, pred, square_root=False) # square_root=True RMSE , false MSE)\n",
        "    return [mape, mae, rmse, mse]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE1aJt203q8-"
      },
      "source": [
        "# MODELOS ESTADÍSTICOS DE FORECASTING (SERIES TEMPORALES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmcrYWa0AJOe"
      },
      "source": [
        "## Definir horizonte de predicción e índice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6gatKfAuAJOe"
      },
      "outputs": [],
      "source": [
        "tam_ypred = 12\n",
        "indice='spei'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuQO9ufgOfqc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "'''cargar archivos con datos de cluster'''\n",
        "cluster = pd.read_csv('https://raw.githubusercontent.com/MandbeZ/TFM_sequia/main/datos/spi_spei/cluster_4.csv',  sep = ',', usecols = {'id', 'cluster', 'estacion'})\n",
        "\n",
        "'''-- Declara DF para almacenar los y_pred,\n",
        "      DF para almacenar los datos de la evaluacion'''\n",
        "df_predts = pd.DataFrame()\n",
        "df_evalts = pd.DataFrame(index = ['MAPE','MAE','RMSE','MSE'])\n",
        "columnas = []\n",
        "\n",
        "\n",
        "'''Escala representa la escala temporal de SPI y SPEI = 3,12'''\n",
        "for escala in [3,12]:\n",
        "        \n",
        "    '''1. Cargar archivo SPI-SPEI'''\n",
        "    cad = indice+str(escala)\n",
        "    datos = cargar_datos('indices_' +cad+ '.csv')\n",
        "\n",
        "    '''2. Procesamiento de datos'''\n",
        "    datos_procesados = procesa_datos(datos)\n",
        "\n",
        "    '''-- Normalizar datos'''\n",
        "    datos_normalizados = normalizar_datos(datos_procesados)\n",
        "    \n",
        "    '''definir los clusters'''\n",
        "    lista_cluster = sorted(cluster['cluster'].unique())\n",
        "    \n",
        "    '''i representa el número de cluster'''\n",
        "    for i in  lista_cluster:\n",
        "        print(f'escala {escala} - cluster {i}')\n",
        "        '''-- Trabajar con un cluster'''\n",
        "        serie = gen_cluster(cluster, i, datos_normalizados)\n",
        "\n",
        "        '''3. Datos de entrenamiento y prueba'''\n",
        "        y_train, y_test = dividir_datos(serie, tamanio = tam_ypred)\n",
        "\n",
        "        '''Definir el horizonte de predicción'''\n",
        "        horizonte_prediccion = horizon_prediccion(y_test)\n",
        "\n",
        "        '''4. Configurar algoritmo, valores ['naive', 'arima', 'ets']'''\n",
        "        predictor = config_algoritmo('arima', estacionalidad = 12, ventana = 48, estrategia = 'mean')\n",
        "\n",
        "        '''5. Ajuste modelo'''\n",
        "        predictor = ajuste(predictor, y_train)\n",
        "\n",
        "        '''6. Predicción'''\n",
        "        y_pred = prediccion(predictor, horizonte_prediccion)\n",
        "\n",
        "        '''7. Evaluación'''\n",
        "        datos_eval = evaluacion_modelo(y_test, y_pred)\n",
        "\n",
        "        '''-- Almacenar todos los y_pred, datos_eval en DFs y nombre de columnas'''\n",
        "        df_predts = pd.concat([df_predts, y_pred.reset_index(drop=True)], axis = 1)\n",
        "        df_evalts[indice+str(escala)+'_c'+str(i)] = datos_eval\n",
        "        columnas.append(indice+str(escala)+'_c'+str(i))\n",
        "\n",
        "'''Cambiar el nombre de las columnas'''\n",
        "df_predts.columns = columnas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3QUfgGIEHi2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "graficar_pred_est(df_predts, 'Modelo Arima')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_6yyPJkvPwS"
      },
      "outputs": [],
      "source": [
        "df_predts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmhH-1KPyD9I"
      },
      "outputs": [],
      "source": [
        "df_evalts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEtL5SNM0afD"
      },
      "source": [
        "# ALGORITMOS DE REGRESIÓN DE APRENDIZAJE AUTOMÁTICO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abda0hD-8AgY"
      },
      "outputs": [],
      "source": [
        "cluster = pd.read_csv('https://raw.githubusercontent.com/MandbeZ/TFM_sequia/main/datos/spi_spei/cluster_4.csv',  sep = ',', usecols = {'id', 'cluster', 'estacion'})\n",
        "\n",
        "'''-- Declara DF para almacenar los y_pred,\n",
        "      DF para almacenar los datos de la evaluacion'''\n",
        "df_pred = pd.DataFrame()\n",
        "df_eval = pd.DataFrame(index = ['MAPE','MAE','RMSE','MSE'])\n",
        "columnas = []\n",
        "\n",
        "'''Definir el horizonte de la prediccion'''\n",
        "tam_ypred = 12\n",
        "\n",
        "for escala in [3,6,12]:\n",
        "        \n",
        "    '''1. Cargar archivo'''\n",
        "    cad = 'spi'+str(escala)\n",
        "    datos = cargar_datos('indices_' +cad+ '.csv')\n",
        "\n",
        "    '''2. Procesamiento de datos'''\n",
        "    datos_procesados = procesa_datos(datos)\n",
        "\n",
        "    '''-- Normalizar datos'''\n",
        "    datos_normalizados = normalizar_datos(datos_procesados)    \n",
        "    \n",
        "    '''definir los clusters'''\n",
        "    lista_cluster = sorted(cluster['cluster'].unique())\n",
        "    for i in lista_cluster:\n",
        "\n",
        "        '''-- Trabajar con un cluster'''\n",
        "        serie = gen_cluster(cluster, i, datos_normalizados)\n",
        "\n",
        "        '''3. Datos de entrenamiento y prueba'''\n",
        "        y_train, y_test = dividir_datos(serie, tamanio = tam_ypred)\n",
        "\n",
        "        '''Definir el horizonte de predicción'''\n",
        "        horizonte_prediccion = horizon_prediccion(y_test)\n",
        "\n",
        "        '''4. Configurar algoritmo de regresión, valores ['DecisionTreeRegressor','RandomForestRegressor', 'KNeighborsRegressor']'''\n",
        "        predictor = config_algoritmoReg('RandomForestRegressor',  ventana = 48, estrategia = 'recursive',estimadores=30, vecinos=11)\n",
        "\n",
        "        '''5. Ajuste modelo'''\n",
        "        predictor = ajuste(predictor, y_train)\n",
        "\n",
        "        '''6. Predicción'''\n",
        "        y_pred = prediccion(predictor, horizonte_prediccion)\n",
        "\n",
        "        '''7. Evaluación'''\n",
        "        datos_eval = evaluacion_modelo(y_test, y_pred)\n",
        "\n",
        "        '''-- Almacenar todos los y_pred, datos_eval en DFs y nombre de columnas'''\n",
        "        df_pred = pd.concat([df_pred, y_pred.reset_index(drop=True)], axis = 1)\n",
        "        df_eval[indice+str(escala)+'_c'+str(i)] = datos_eval\n",
        "        columnas.append(indice+str(escala)+'_c'+str(i))\n",
        "        \n",
        "'''Cambiar el nombre de las columnas'''\n",
        "df_pred.columns = columnas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4IQZxviIGn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "graficar_pred_est(df_pred, 'Modelo RandomForestRegressor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLnXKRB91QBb"
      },
      "outputs": [],
      "source": [
        "df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFXXCZxN1QYv"
      },
      "outputs": [],
      "source": [
        "df_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Mmuby7ikut"
      },
      "outputs": [],
      "source": [
        "t1 = pd.Timestamp.now()\n",
        "t1-t0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "4.0.Modelo_Univariante_ARIMA_RF__SPI_SPEI(Por_cluster).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "tfm",
      "language": "python",
      "name": "tfm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}