{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MandbeZ/TFM_sequia/blob/main/notebooks/4_2_0_Modelo_SerieTemporal_MLP__SPI_SPEI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY2_K3CUHEtp"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow==2.1\n",
        "%pip install keras-tcn\n",
        "%pip install numpy==1.19.2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7u3FF2ZWjS4"
      },
      "outputs": [],
      "source": [
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
        "simplefilter(action=\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k_JBnN4i3UC"
      },
      "source": [
        "## Importar librerías básicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggv3OWS9InjY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB0GhvuJqbMs"
      },
      "source": [
        "## Importar librerías de Tensor Flow y Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpw-rln7i0SC",
        "outputId": "cbfb8114-92e2-4a7c-dcf0-1d99a95e26f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, LSTM, Input #, GRU\n",
        "from tensorflow.keras import backend as K\n",
        "from tcn import TCN\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error,mean_absolute_percentage_error\n",
        "from math import sqrt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6-Q9U6djWmH"
      },
      "source": [
        "## Definición de Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GVxvxzojTRR"
      },
      "outputs": [],
      "source": [
        "#Colores para los plot\n",
        "colores = plt.get_cmap('Set1', 30)\n",
        "\n",
        "def cargar_datos(archivo, lista_cluster, c):\n",
        "    data = pd.read_csv('https://raw.githubusercontent.com/MandbeZ/TFM_sequia/main/datos/spi_spei/' + archivo,  sep = ',', parse_dates=True)\n",
        "    estaciones = lista_cluster[lista_cluster['cluster'] == c]\n",
        "    nom_cols = [col for est in estaciones['id'] for col in data.columns if str(est) in col]\n",
        "    nom_cols.append('fecha')\n",
        "    datos = data[nom_cols]\n",
        "    return datos\n",
        "\n",
        "'''Llevar la  fecha a índice, configurar como periodo'''\n",
        "def procesa_datos(data):\n",
        "    data['fecha'] = pd.to_datetime(data['fecha'])\n",
        "    data = data.dropna()\n",
        "    data = data.set_index('fecha')\n",
        "    data.index = data.index.to_period('M')\n",
        "    return data\n",
        "\n",
        "'''Se normalizan los datos Min-Max: [0,1]'''\n",
        "def normalizar_datos(data,param):\n",
        "    return (data - param['min']) / ( param['max'] - param['min'])\n",
        "\n",
        "def plotear(x, y, y_pred=None):\n",
        "  fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
        "  ax.plot(x, 'o-',  c=colores(1), markersize=3.5, label='Entrenamiento(Train)')\n",
        "  ax.plot([m for m in range(x.shape[0],x.shape[0]+y.shape[0])], y, 'x-', c=colores(5), markersize=3.5, label='Validación(Test)')\n",
        "  if y_pred is not None:\n",
        "    ax.plot([m for m in range(x.shape[0],x.shape[0]+y.shape[0])], y_pred, 'o-', c=colores(8), markersize=3.5, label='Predicción(Pred)')\n",
        "  ax.legend()\n",
        "\n",
        "def evaluar(test,pred):\n",
        "  mape = mean_absolute_percentage_error(test, pred)\n",
        "  mae = mean_absolute_error(test, pred)  \n",
        "  mse = mean_squared_error(test, pred) # square=True MSE , false RMSE\n",
        "  rmse = sqrt(mean_squared_error(test, pred)) # square=True MSE , false RMS\n",
        "  return [mae,mape,rmse,mse]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpDLIVP4gcAC"
      },
      "outputs": [],
      "source": [
        "'''Cargar archivo con los clusters'''\n",
        "cluster = pd.read_csv('https://raw.githubusercontent.com/MandbeZ/TFM_sequia/main/datos/spi_spei/cluster_4.csv',  sep = ',', usecols = {'id', 'cluster'})\n",
        "\n",
        "'''Definir el horizonte de la prediccion'''\n",
        "hpred = 12  #Horizonte de Predicción \n",
        "\n",
        "'''Definir los hiperparamentros de RN'''\n",
        "lista_epocas = [100]  #[60,100,140]\n",
        "lista_tbatch = [32,64] #[32,64,128]\n",
        "error='mse' #mae o mse\n",
        "\n",
        "'''lista de neuronas para las capas ocultas'''\n",
        "lista_c1 = [24,30,40]\n",
        "lista_c2 = [24,30,40]\n",
        "\n",
        "'''Definir otras variables de los datos SPI-SPEI'''\n",
        "lista_indice = ['spi','spei']\n",
        "lista_escala = [12]\n",
        "lista_cluster = [0,1,2,3]\n",
        "\n",
        "\n",
        "'''Definir DF de evaluacion y prediccion'''\n",
        "mlp_eval = pd.DataFrame(index=['MAE','MAPE' ,'RMSE','MSE'])\n",
        "mlp_pred = pd.DataFrame()\n",
        "\n",
        "lista_ventana = [48,72]  #Pasado histórico [12,24,36,48,72]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfPuXstDC7CY"
      },
      "source": [
        "# Perceptrón Multicapa MLP (Multi-Layer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEI9pATNhDDP"
      },
      "outputs": [],
      "source": [
        "'''Perceptrón con dos capas'''\n",
        "for lind in lista_indice:\n",
        "  for lesc in lista_escala:\n",
        "    for lclu in lista_cluster:\n",
        "      # Cargar datos\n",
        "      datos = cargar_datos('indices_'+lind+str(lesc)+'.csv', cluster,lclu)\n",
        "      datos_p = procesa_datos(datos)\n",
        "      '''Diferenciar las series para spi y spei 12'''\n",
        "      datos_p_d = datos_p.diff()\n",
        "      datos_p=datos_p_d.dropna()\n",
        "\n",
        "      '''División del DataSet en Entramiento y test'''\n",
        "      train = datos_p.iloc[:-hpred]\n",
        "      test = datos_p.iloc[-hpred:]\n",
        "\n",
        "      '''Normalizar datos'''\n",
        "      s_train = []\n",
        "      for i in list(train):\n",
        "          s_train.append(train[i].tolist())\n",
        "\n",
        "      s_test = []\n",
        "      for i in list(test):\n",
        "          s_test.append(test[i].tolist())\n",
        "\n",
        "      s_train = np.asarray(s_train)\n",
        "      s_test = np.asarray(s_test)\n",
        "\n",
        "      train_norm = []\n",
        "      #Obtener los parámetros de normalización de train\n",
        "      lista_param = []\n",
        "\n",
        "      for s in s_train:\n",
        "        params = {}\n",
        "        params['max'] = s.max()\n",
        "        params['min'] = s.min()\n",
        "        lista_param.append(params)\n",
        "        norm = normalizar_datos(s, params)\n",
        "        train_norm.append(norm)\n",
        "      # Aplicar los parámetros de normalización al test \n",
        "      test_norm = []\n",
        "      for x, s in enumerate(s_test):\n",
        "        params = lista_param[x]\n",
        "        norm = normalizar_datos(s,params)\n",
        "        test_norm.append(norm)\n",
        "      test_norm[len(test_norm)-1]\n",
        "\n",
        "\n",
        "      for ventana in lista_ventana:\n",
        "        '''ventana movil'''\n",
        "        x_train, y_train = [], []\n",
        "        x_test, y_test = [], []\n",
        "\n",
        "        for i, ts in enumerate(train_norm):\n",
        "          # Train data\n",
        "          ts_x_train, ts_y_train = [], []\n",
        "          for j in range(0, ts.shape[0] - hpred + 1):\n",
        "              indices = range(j - ventana, j, 1)\n",
        "              \n",
        "              ts_x_train.append(np.reshape(ts[indices], (ventana, 1)))\n",
        "              ts_y_train.append(ts[j:j + hpred])\n",
        "          x_train.extend(np.asarray(ts_x_train))\n",
        "          y_train.extend(np.asarray(ts_y_train))\n",
        "          # Test data\n",
        "          ts_x_test = np.reshape(np.asarray(ts[-ventana:]), (ventana, 1))\n",
        "          ts_y_test=  test_norm[i]\n",
        "          x_test.extend(np.asarray([ts_x_test]))\n",
        "          y_test.extend(np.asarray([ts_y_test]))\n",
        "          \n",
        "        x_train, y_train = np.asarray(x_train), np.asarray(y_train)\n",
        "        x_test, y_test = np.asarray(x_test), np.asarray(y_test, dtype='float32')\n",
        "\n",
        "        # Fijar la semilla para los experimentos\n",
        "        tf.keras.backend.clear_session()\n",
        "        np.random.seed(1)\n",
        "        tf.random.set_seed(1)\n",
        "        random.seed(1)\n",
        "\n",
        "        for epocas in lista_epocas:\n",
        "          for tbatch in lista_tbatch:\n",
        "            # print(f'Ventana:{ventana}, epoca:{epocas}, tbatch:{tbatch}')\n",
        "\n",
        "            for x1 in lista_c1:\n",
        "              if x1<=ventana:\n",
        "                for x2 in lista_c2:\n",
        "                  if x2<=x1:\n",
        "                    # print(f'x1:{x1}, x2:{x2}, x3:{x3}')\n",
        "                    print('MLP2_'+lind+str(lesc)+'_c'+str(lclu)+'_v'+str(ventana)+'_e'+str(epocas)+'_b'+str(tbatch)+'.'+str(x1)+'.'+str(x2))\n",
        "\n",
        "                    ''' PERCEPTRON MULTICAPA MLP'''\n",
        "                    inp = Input(shape=x_train.shape[-2:])\n",
        "                    x = Flatten()(inp)\n",
        "                    x = Dense(x1)(x)\n",
        "                    x = Dense(x2)(x)\n",
        "                    x = Dense(hpred)(x)\n",
        "                    model = keras.Model(inputs=inp, outputs=x)\n",
        "                    model.compile(optimizer='adam', loss=error)\n",
        "                    print(model.summary())\n",
        "\n",
        "                    history = model.fit(x_train, y_train,\n",
        "                              batch_size=tbatch,\n",
        "                              epochs=epocas,\n",
        "                              verbose=1,\n",
        "                              validation_data=(x_test, y_test))\n",
        "                    \n",
        "                    nom_columna = 'MLP2'+lind+str(lesc)+'_c'+str(lclu)+'_v'+str(ventana)+'_e'+str(epocas)+'_b'+str(tbatch)+'.'+str(x1)+'.'+str(x2)\n",
        "                    # Graficas de entrenamiento y validación\n",
        "                    plt.figure()\n",
        "                    plt.title('MLP2_'+lind+str(lesc)+'_c'+str(lclu)+'_v'+str(ventana)+'_e'+str(epocas)+'_b'+str(tbatch)+'.'+str(x1)+'.'+str(x2))\n",
        "                    plt.xlabel('Epocas')\n",
        "                    plt.ylabel('Pérdida')\n",
        "                    plt.plot(history.history['loss'])\n",
        "                    plt.plot(history.history['val_loss'])\n",
        "                    plt.legend(['Entrenamiento', 'Validación'])\n",
        "                    plt.savefig('/content/gdrive/My Drive/Resultados_MLP/SPI_SPEI12_dif/'+nom_columna+'.png', dpi=300 , transparent=False)\n",
        "                    # plt.show()\n",
        "\n",
        "                    predicciones = model.predict(x_test)\n",
        "                    \n",
        "                    # mlp_eval[nom_columna]=evaluar(y_test,predicciones)\n",
        "                    with open('/content/gdrive/My Drive/Resultados_MLP/SPI_SPEI12_dif/eval_'+lind+str(lesc)+'_c'+str(lclu)+'_MPL2_.txt', 'a') as f:\n",
        "                      f.write(nom_columna+','+str(evaluar(y_test,predicciones))+'\\n')\n",
        "                    # mlt = pd.DataFrame(predicciones, index=['mlt'+datos_p.columns.astype(str)]).T\n",
        "                    # mlp_pred=pd.concat([mlp_pred,mlt],axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guJlz0NOitkA"
      },
      "source": [
        "# 1. Cargar los datos de SPI o SPEI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhekML6bk1ht"
      },
      "outputs": [],
      "source": [
        "# '''Cargar archivo con los clusters'''\n",
        "# cluster = pd.read_csv('https://raw.githubusercontent.com/MandbeZ/TFM_sequia/main/datos/spi_spei/cluster_4.csv',  sep = ',', usecols = {'id', 'cluster'})\n",
        "\n",
        "# '''Cargar archivo y extraer un cluster'''\n",
        "# datos = cargar_datos('indices_spi3.csv', cluster,3)\n",
        "\n",
        "\n",
        "# datos_p = procesa_datos(datos)\n",
        "# '''Diferenciar las series'''\n",
        "# datos_p_d = datos_p.diff()\n",
        "# datos_p=datos_p_d.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am94rKVsk5_E"
      },
      "source": [
        "# 2. Definir horizonte de predicción y dividir el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F__yPtuKis9H"
      },
      "outputs": [],
      "source": [
        "# '''Definir el horizonte de la prediccion'''\n",
        "# hpred = 12\n",
        "# '''División del DataSet en Entramiento y test'''\n",
        "# train = datos_p.iloc[:-hpred]\n",
        "# test = datos_p.iloc[-hpred:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1yAlPJo1oit"
      },
      "source": [
        "# 3. Normalizar los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo9-Lo0U1Uob"
      },
      "outputs": [],
      "source": [
        "# s_train = []\n",
        "# for i in list(train):\n",
        "#     s_train.append(train[i].tolist())\n",
        "\n",
        "# s_test = []\n",
        "# for i in list(test):\n",
        "#     s_test.append(test[i].tolist())\n",
        "\n",
        "# s_train = np.asarray(s_train)\n",
        "# s_test = np.asarray(s_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAYF--HLMFzy"
      },
      "outputs": [],
      "source": [
        "# train_norm = []\n",
        "# #Obtener los parámetros de normalización de train\n",
        "# lista_param = []\n",
        "\n",
        "# for s in s_train:\n",
        "#   params = {}\n",
        "#   params['max'] = s.max()\n",
        "#   params['min'] = s.min()\n",
        "#   lista_param.append(params)\n",
        "#   norm = normalizar_datos(s, params)\n",
        "#   train_norm.append(norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-NxMEp6lSlg"
      },
      "outputs": [],
      "source": [
        "# # Aplicar los parámetros de normalización al test \n",
        "# test_norm = []\n",
        "# for x, s in enumerate(s_test):\n",
        "#   params = lista_param[x]\n",
        "#   norm = normalizar_datos(s,params)\n",
        "#   test_norm.append(norm)\n",
        "# test_norm[len(test_norm)-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVOJH9ih5BP"
      },
      "source": [
        "# 4. Definir tamaño de ventana (pasado histórico)  y horizonte de predicción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jae717xuhhHj"
      },
      "outputs": [],
      "source": [
        "# ventana = 240  #Pasado histórico\n",
        "# hpred = 12  #Horizonte de Predicción "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNqIj3xRiCmb"
      },
      "source": [
        "# 5. Estategia de Ventana Móvil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ6rO2rghlAJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# x_train, y_train = [], []\n",
        "# x_test, y_test = [], []\n",
        "\n",
        "# for i, ts in enumerate(train_norm):\n",
        "#   # Train data\n",
        "#   ts_x_train, ts_y_train = [], []\n",
        "#   for j in range(0, ts.shape[0] - hpred + 1):\n",
        "#       indices = range(j - ventana, j, 1)\n",
        "      \n",
        "#       ts_x_train.append(np.reshape(ts[indices], (ventana, 1)))\n",
        "#       ts_y_train.append(ts[j:j + hpred])\n",
        "#   x_train.extend(np.asarray(ts_x_train))\n",
        "#   y_train.extend(np.asarray(ts_y_train))\n",
        "#   # Test data\n",
        "#   ts_x_test = np.reshape(np.asarray(ts[-ventana:]), (ventana, 1))\n",
        "#   ts_y_test=  test_norm[i]\n",
        "#   x_test.extend(np.asarray([ts_x_test]))\n",
        "#   y_test.extend(np.asarray([ts_y_test]))\n",
        "  \n",
        "\n",
        "# x_train, y_train = np.asarray(x_train), np.asarray(y_train)\n",
        "# x_test, y_test = np.asarray(x_test), np.asarray(y_test, dtype='float32')\n",
        "\n",
        "# print(\"DATOS DE ENTRENAMIENTO\")\n",
        "# print(\"x_train\", x_train.shape)\n",
        "# print(\"y_train\", y_train.shape)\n",
        "# print()\n",
        "# print(\"DATOS DE VALIDACIÓN\")\n",
        "# print(\"x_test\", x_test.shape)\n",
        "# print(\"y_test\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOp2w0OMps3m"
      },
      "outputs": [],
      "source": [
        "# # from collections import defaultdict\n",
        "# # resultados = defaultdict(lambda: {})\n",
        "\n",
        "# # Definir DF de evaluacio y prediccion\n",
        "# eval = pd.DataFrame(index=['MAE','MAPE' ,'RMSE','MSE'])\n",
        "# pred = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw9mdIPHpzL8"
      },
      "outputs": [],
      "source": [
        "# #Definición de Hiperparámetros para modelo de Perceptrón multicapa\n",
        "# error='mae'\n",
        "# tbatch=128\n",
        "# epocas=135\n",
        "# # Fijar la semilla para los experimentos\n",
        "# np.random.seed(1)\n",
        "# tf.random.set_seed(1)\n",
        "# random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8vc2oSFCj9w"
      },
      "outputs": [],
      "source": [
        "# inp = Input(shape=x_train.shape[-2:])\n",
        "# x = Flatten()(inp)\n",
        "# x = Dense(16)(x)\n",
        "# x = Dense(32)(x)\n",
        "# x = Dense(32)(x)\n",
        "# x = Dense(hpred)(x)\n",
        "# model = keras.Model(inputs=inp, outputs=x)\n",
        "\n",
        "# model.compile(optimizer='adam', loss=error)\n",
        "# print(model.summary())\n",
        "\n",
        "# history = model.fit(x_train, y_train,\n",
        "#           batch_size=tbatch,\n",
        "#           epochs=epocas,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))\n",
        "# # Graficas de entrenamiento y validación\n",
        "# plt.figure()\n",
        "# plt.xlabel('Epocas')\n",
        "# plt.ylabel('Pérdida')\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.legend(['Entrenamiento', 'Validación'])\n",
        "\n",
        "# predicciones = model.predict(x_test)\n",
        "# # mae = mean_absolute_error(y_test, predicciones)\n",
        "# # resultados['MAE']['MLP'] = mae\n",
        "# # resultados['Y_PRED']['MLP'] = predicciones\n",
        "# # print(pd.DataFrame(resultados)['MAE'])\n",
        "# # Evaluar MLP\n",
        "# eval['MLP']=evaluar(y_test,predicciones)\n",
        "# mlt = pd.DataFrame(predicciones, index=['mlt'+datos_p.columns.astype(str)]).T\n",
        "# pred=pd.concat([pred,mlt],axis=1)\n",
        "\n",
        "# for x, y, y_pred in zip(x_test, y_test, predicciones):\n",
        "#   plotear(x,y,y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "4.2.0. Modelo_SerieTemporal_MLP__SPI_SPEI.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}